{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% Imports\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import subprocess as sp\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from ellipse import LsqEllipse\n",
    "import cv2\n",
    "import math\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def oe_events_parser(open_ephys_csv_path, channel_names, export_path=None):\n",
    "    \"\"\"\n",
    "    :param open_ephys_csv_path: The path to an open ephys analysis tools exported csv (using TrialReporter.ipynb)\n",
    "    :param channel_names: a dictionary of the form -\n",
    "                    { 1 : 'channel name' (L_eye_camera)\n",
    "                      2 : 'channel name' (Arena_TTL)\n",
    "                      etc..\n",
    "                    }\n",
    "    :param export_path: default None, if a path is specified a csv file will be saved\n",
    "    :returns open_ephys_events: a pandas DataFrame object where each column has the ON events of one channel\n",
    "                                and has a title from channel_names\n",
    "    :returns open_ephys_off_events: same but for the OFF states (only important for the logical start-stop signal)\n",
    "    \"\"\"\n",
    "\n",
    "    # Infer the active channels:\n",
    "    df = pd.read_csv(open_ephys_csv_path)\n",
    "    channels = df['channel'].to_numpy(copy=True)\n",
    "    channels = np.unique(channels)\n",
    "    df_onstate = df[df['state']==1] #cut the df to represent only rising edges\n",
    "    df_offstate = df[df['state']==0] # This one is important for the ON/OFF signal of the arena\n",
    "    list = []\n",
    "    off_list= []\n",
    "    for chan in channels: #extract a pandas series of the ON stats timestamps for each channel\n",
    "        Sname = channel_names[chan]\n",
    "        s = pd.Series(df_onstate['timestamp'][df_onstate['channel'] == chan], name=Sname)\n",
    "        offs = pd.Series(df_offstate['timestamp'][df_offstate['channel'] == chan], name=Sname)\n",
    "        list.append(s)\n",
    "        off_list.append(offs)\n",
    "    open_ephys_events = pd.concat(list, axis=1)\n",
    "    open_ephys_off_events = pd.concat(off_list, axis=1)\n",
    "    if export_path is not None :\n",
    "        if not export_path in os.listdir(open_ephys_csv_path.split('events.csv')[0][:-1]):\n",
    "            open_ephys_events.to_csv(export_path)\n",
    "    return open_ephys_events , open_ephys_off_events\n",
    "\n",
    "def convert_h264_mp4(path):\n",
    "    files_to_convert = glob.glob(path + r'\\**\\*.h264', recursive=True)\n",
    "    converted_files = glob.glob(path + r'\\**\\*.mp4', recursive=True)\n",
    "    for file in files_to_convert:\n",
    "        fps = file[file.find('hz') - 2:file.find('hz')]\n",
    "        if len(fps) != 2:\n",
    "            fps = 60\n",
    "            print('could not determine fps, using 60...')\n",
    "        if not str(fr'{file[:-5]}.mp4') in converted_files:\n",
    "            sp.run(f'MP4Box -fps {fps} -add {file} {file[:-5]}.mp4')\n",
    "            print(fr'{file} converted ')\n",
    "        else:\n",
    "            print(f'The file {file[:-5]}.mp4 already exists, no conversion necessary')\n",
    "\n",
    "def validate_no_framedrop(path):\n",
    "    videos_to_inspect = glob.glob(path + r'\\**\\*.mp4', recursive=True)\n",
    "    timestamps_to_inspect = glob.glob(path + r'\\**\\*.csv', recursive=True)\n",
    "    for vid in range(len(videos_to_inspect)):\n",
    "        timestamps = pd.read_csv(timestamps_to_inspect[vid])\n",
    "        num_reported = timestamps.shape[0]\n",
    "        cap = cv2.VideoCapture(videos_to_inspect[vid])\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f'The video named {os.path.split(videos_to_inspect[vid])[1]} has reported {num_reported} frames '\n",
    "              f'and has {length} frames, it has dropped {num_reported - length} frames')\n",
    "        cap.release()\n",
    "\n",
    "def stamp_diff_videos(path_to_stamp,stamp):\n",
    "    videos_to_stamp = glob.glob(path_to_stamp + r'\\**\\*.mp4', recursive=True)\n",
    "    for vid in videos_to_stamp:\n",
    "        os.rename(vid, fr'{vid[:-4]}_{stamp}{vid[-4:]}')\n",
    "\n",
    "def get_frame_timeseries(df,channel):\n",
    "    index_range = range(0,len(df[channel][df[channel].notna()]))\n",
    "    timeseries = pd.Series(df[channel][df[channel].notna()])\n",
    "    timeseries = pd.Series(timeseries.values, index=index_range, name=channel)\n",
    "    return timeseries\n",
    "\n",
    "def get_closest_frame(timestamp, vid_timeseries, report_acc=None):\n",
    "    \"\"\"\n",
    "    This function extracts a frame from a series so that it is as close as possible to a given timestamp\n",
    "    :param timestamp: The time to match a frame to\n",
    "    :param vid_timeseries: The time frames series to look at for a match\n",
    "    :param report_acc: if set to 1, will report the accuracy of the match\n",
    "    :return: index_of_lowest_diff , accuracy of match(if requested)\n",
    "    \"\"\"\n",
    "    array = np.abs((vid_timeseries.to_numpy())-timestamp)\n",
    "    index_of_lowest_diff = np.argmin(array)\n",
    "    if report_acc == 1:\n",
    "        accuracy = abs(vid_timeseries[index_of_lowest_diff] - timestamp)\n",
    "        return index_of_lowest_diff, accuracy\n",
    "    else:\n",
    "        return index_of_lowest_diff\n",
    "\n",
    "def arena_video_initial_thr(vid_path, threshold_value, show_frames=False):\n",
    "    \"\"\"\n",
    "        This function works through an arena video to determine where the LEDs are on and when off\n",
    "        :param threshold_value: value of the frame threshold\n",
    "        :param show_frames: if true will show the video after thresholding\n",
    "        :param  vid_path: Path to video. When ShowFrames is True a projection of the frames after threshold is presented\n",
    "\n",
    "        :return: np.array with frame numbers and mean values after threshold\n",
    "        \"\"\"\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    all_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    flag = 0\n",
    "    i = 0\n",
    "    mean_values = []\n",
    "    indexes = []\n",
    "    while flag == 0:\n",
    "        print('Frame number {} of {}'.format(i, all_frames), end='\\r', flush=True)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        grey[grey < threshold_value] = 0\n",
    "        mean_values.append(np.mean(grey))\n",
    "        indexes.append(i)\n",
    "        if show_frames:\n",
    "            cv2.imshow('Thresholded_Frames', grey)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        i += 1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    frame_val = np.array((indexes, mean_values))\n",
    "    return frame_val\n",
    "\n",
    "def produce_frame_val_list(vid_paths,threshold_value):\n",
    "    \"\"\"\n",
    "    :param vid_paths: a list of str paths to videos for analysis\n",
    "    :param threshold_value: the threshold to use in order to concentrate on LEDs\n",
    "    :return: frame_val_list: a list of mean pixel values for each frame after threshold\n",
    "    \"\"\"\n",
    "    frame_val_list = []\n",
    "    for vid in vid_paths:\n",
    "        print(f'working on video {vid}')\n",
    "        frame_val = arena_video_initial_thr(vid, threshold_value)\n",
    "        frame_val_list.append(frame_val)\n",
    "    print(f'done, frame_val_list contains {len(frame_val_list)} objects',flush=True)\n",
    "\n",
    "    return frame_val_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Function definitions\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook should be the main sequence for incoming experimental data - there are prerequisites for a functional process:\n",
    "- the process is designed to work block-by-block\n",
    "- Data will be arranged into block folders under animal folders, where each block contains the next structure:\n",
    "\n",
    "                                       /----> arena_videos  ->[config.yaml , info.yaml] videos -> [video files, output.log] timestamps -> [csv of timestamps]\n",
    "\n",
    "Animal_x ->date(xx_xx_xxxx) -> block_x -----> eye_videos >> LE\\RE -> video folder with name -> [video.h264 , video.mp4 , params.json , timestamps.csv]\n",
    "\n",
    "                                       \\----> oe_files >> date_time(xxxx_xx_xx_xx-xx-xx) --> [events.csv] internal open ephys structure from here (NWB format only!!!)\n",
    "                                                                                             /////////////////////\n",
    "                                                                                         TODO: internal parsing of this file\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Sketch\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "animal_number = '0'\n",
    "experiment_date = '24_03_2021'\n",
    "block = '9'\n",
    "path = rf'D:\\experiments\\Animal_{animal_number}\\{experiment_date}\\block_{block}'\n",
    "print(path)\n",
    "#Arena\n",
    "arvid_path = path + r'\\arena_videos\\videos\\*.mp4'\n",
    "arena_videos = glob.glob(path + r'\\arena_videos\\videos\\*.mp4')\n",
    "print(arena_videos)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Define block path\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ar_vidnames = []\n",
    "for vid in arena_videos:\n",
    "    vidname = vid.split(sep=\"\\\\\") ; vidname = vidname[len(vidname)-1]; vidname=vidname.split(sep='.') ; vidname=vidname[0]\n",
    "    ar_vidnames.append(vidname)\n",
    "print(ar_vidnames)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% extract arena video names\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vid_path = path + r'\\eye_videos'\n",
    "print('converting videos...')\n",
    "convert_h264_mp4(vid_path)\n",
    "print('Validating videos...')\n",
    "validate_no_framedrop(vid_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%Eye_Vids initial conversion & validation (RUN ONLY ONCE!!!)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('stamping LE video')\n",
    "stamp_diff_videos(vid_path+r'\\LE' , 'LE')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% RUN ONLY ONCE!!!!\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le_video = glob.glob(path + r'\\eye_videos\\LE\\**\\*.mp4')\n",
    "re_video = glob.glob(path + r'\\eye_videos\\RE\\**\\*.mp4')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% find the converted eye_vids\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "channeldict = {\n",
    "    5 : 'L_eye_TTL',\n",
    "    6 : 'Arena_TTL',\n",
    "    7 : 'Logical ON/OFF',\n",
    "    8 : 'R_eye_TTL'\n",
    "}\n",
    "exp_date_time = os.listdir(fr'{path}\\oe_files')[0]\n",
    "oe_events , oe_off_events = oe_events_parser(path + rf'\\oe_files\\{exp_date_time}\\events.csv', channeldict, export_path=path + rf'\\oe_files\\{exp_date_time}\\parsed_events.csv')\n",
    "ts_list = []\n",
    "for chan in list(oe_events.columns):\n",
    "    ts = get_frame_timeseries(oe_events,str(chan))\n",
    "    ts_list.append(ts)\n",
    "logical_off_series = get_frame_timeseries(oe_off_events,'Logical ON/OFF')\n",
    "block_start_time = ts_list[2].values[0]\n",
    "del ts_list[2]\n",
    "block_end_time = logical_off_series.values[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Analyze the Open Ephys file and extract the csv, then the timeseries for each channel\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Arena video start: {block_start_time} \\n'\n",
    "      f'Arena video end: {block_end_time} \\n'\n",
    "      f'Block length: {block_end_time - block_start_time} Seconds')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Temporal Orientation + sanity check\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Arena\n",
    "arena_ff = ts_list[1].values[ts_list[1]>block_start_time][0]\n",
    "arena_first_index = ts_list[1][ts_list[1] == arena_ff].index\n",
    "arena_lf = ts_list[1].values[ts_list[1]<block_end_time][-1]\n",
    "arena_last_index = ts_list[1][ts_list[1] == arena_lf].index\n",
    "arena_sync_s = pd.Series(ts_list[1][arena_first_index.asi8[0] : arena_last_index.asi8[0]])\n",
    "\n",
    "#Left eye\n",
    "le_ff = ts_list[0].values[0]\n",
    "le_first_index = ts_list[0][ts_list[0] == le_ff].index\n",
    "le_lf = ts_list[0].values[-1]\n",
    "le_last_index = ts_list[0][ts_list[0] == le_lf].index\n",
    "le_sync_s = ts_list[0]\n",
    "\n",
    "#Right eye\n",
    "re_ff = ts_list[2].values[0]\n",
    "re_first_index = ts_list[2][ts_list[2] == re_ff].index\n",
    "re_lf = ts_list[2].values[-1]\n",
    "re_last_index = ts_list[2][ts_list[2] == re_lf].index\n",
    "re_sync_s = ts_list[2]\n",
    "\n",
    "sync_time_starts = max([arena_ff, le_ff, re_ff])\n",
    "sync_time_ends = min([arena_lf, le_lf, re_lf])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Find the first frame (ff) and last frame (lf) for every video input + construct series for synchronization\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Define Anchor signal\n",
    "anchor_signal = np.arange(sync_time_starts, sync_time_ends, 1/60)\n",
    "\n",
    "#Create the DataFrame\n",
    "sync_vids = pd.DataFrame(data=None, index=range(len(anchor_signal)), columns=['Left_eye','Arena','Arena_VideoFrame','Right_eye'])\n",
    "accuracy_report = pd.DataFrame(data=None, index=range(len(anchor_signal)), columns=['Left_eye','Arena','Right_eye'])\n",
    "# define dictionary for timestamp retrieval\n",
    "ts_dict = {'Left_eye': ts_list[0],\n",
    "           'Arena': ts_list[1],\n",
    "           'Right_eye':ts_list[2]}\n",
    "\n",
    "#Iterate over the length of the dataframe and fit\n",
    "for frame in range(len(anchor_signal)):\n",
    "    anchor_time = anchor_signal[frame]\n",
    "    if frame % 50 == 0:\n",
    "        print(f'frame {frame} out of {len(anchor_signal)}', end='\\r', flush=True)\n",
    "    for vid in ['Left_eye', 'Right_eye', 'Arena']:\n",
    "        f,a = get_closest_frame(anchor_time, ts_dict[vid], report_acc=1)\n",
    "        sync_vids[vid][frame] = f\n",
    "        accuracy_report[vid][frame] = a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Align Frames in synchronization DataFrame (use shortest_vid_name as the anchor)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sync_vids.Arena_VideoFrame = sync_vids.Arena - (arena_first_index.asi8[0] + 70)\n",
    "sync_vids.Right_eye = sync_vids.Right_eye + 0\n",
    "sync_vids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%Arena sync step (becuase the TTLs are constant + 100 lag between acquisition start and TTL start)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sync_vids_copy = sync_vids\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eye_tracking_analysis(dlc_video_analysis_csv):\n",
    "    \"\"\"\n",
    "    :param dlc_video_analysis_csv: the csv output of a dlc analysis of one video, already read by pandas with header=1\n",
    "    :param bodyparts_list: a list of bodyparts as described in the dlc csv (i.e ['Pupil_12', 'Pupil_6'....])\n",
    "    :returns ellipse_df: a DataFrame of ellipses parameters (center, width, height, phi, size) for each video frame\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data = dlc_video_analysis_csv\n",
    "    ellipses = []\n",
    "    for row in range(1, len(data)-1):\n",
    "        x_values = np.array(list([float(data['Pupil_12'][row]),float(data['Pupil_1'][row]),float(data['Pupil_3'][row]),float(data['Pupil_4'][row]),float(data['Pupil_6'][row]),float(data['Pupil_8'][row]),float(data['Pupil_10'][row])]))\n",
    "        y_values = np.array(list([float(data['Pupil_12.1'][row]),float(data['Pupil_1.1'][row]),float(data['Pupil_3.1'][row]),float(data['Pupil_4.1'][row]),float(data['Pupil_6.1'][row]),float(data['Pupil_8.1'][row]),float(data['Pupil_10.1'][row])]))\n",
    "        X = np.c_[x_values,y_values]\n",
    "\n",
    "        el = LsqEllipse().fit(X)\n",
    "        center, width, height, phi = el.as_parameters()\n",
    "        ellipses.append([center,width,height,phi])\n",
    "        if row % 50 == 0:\n",
    "           print(f'just finished with {row} out of {len(data)-1}', end='\\r',flush=True)\n",
    "    ellipse_df = pd.DataFrame(columns = ['center', 'width', 'height', 'phi'], data = ellipses)\n",
    "    a = np.array(ellipse_df['height'][:])\n",
    "    b = np.array(ellipse_df['width'][:])\n",
    "    ellipse_size_per_frame = a*b*math.pi\n",
    "    ellipse_size_per_frame\n",
    "    ellipse_df['ellipse_size'] = ellipse_size_per_frame\n",
    "    print('Done')\n",
    "    return ellipse_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ellipse function\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le_csv = pd.read_csv(glob.glob(path + r'\\eye_videos\\LE\\**\\*DLC*.csv')[0],header=1)\n",
    "\n",
    "re_csv = pd.read_csv(glob.glob(path + r'\\eye_videos\\RE\\**\\*DLC*.csv')[0],header=1)\n",
    "\n",
    "\n",
    "le_ellipses = eye_tracking_analysis(le_csv)\n",
    "print('\\n')\n",
    "re_ellipses = eye_tracking_analysis(re_csv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get paths for the dlc files of each eye\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,7])\n",
    "plt.plot(anchor_signal[0:25724], le_ellipses.ellipse_size[sync_vids.Left_eye.values[0:25724]])\n",
    "plt.plot(anchor_signal[0:25724], re_ellipses.ellipse_size[sync_vids.Right_eye.values[0:25724]])\n",
    "_ = plt.ylim(1000,7000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arena_frame_val_list = produce_frame_val_list(arena_videos,250)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% analyze arena video for led-off frames\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l_eye_frame_val_list = produce_frame_val_list(le_video, 30)\n",
    "r_eye_frame_val_list = produce_frame_val_list(re_video,30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% analyze eye videos\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l_eye_values = stats.zscore(l_eye_frame_val_list[0][1])\n",
    "r_eye_values = stats.zscore(r_eye_frame_val_list[0][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create the l/r eyes value df\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sync_vids.Arena_VideoFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arena_brightness_df = pd.DataFrame(index = anchor_signal)\n",
    "for ind, vid in enumerate(ar_vidnames):\n",
    "    vid_val_arr = stats.zscore(arena_frame_val_list[ind][1])\n",
    "    sync_list = sync_vids.Arena_VideoFrame.astype(int)\n",
    "    sync_list[sync_list >= len(vid_val_arr)] = len(vid_val_arr)-1\n",
    "    arena_brightness_df.insert(loc=0, column=str(vid), value=vid_val_arr[sync_list])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create synchronization dataframes\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eye_brightness_df = pd.DataFrame(index=anchor_signal)\n",
    "eye_brightness_df.insert(loc=0,column='left_eye',value=l_eye_values[sync_vids.Left_eye.values.astype(int)[0:len(anchor_signal)]])\n",
    "eye_brightness_df.insert(loc=0,column='right_eye',value=r_eye_values[sync_vids.Right_eye.values.astype(int)[0:len(anchor_signal)]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arena_brightness_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sec = 3600\n",
    "x = 3\n",
    "region = range(1020+x*sec, 1150+x*sec)\n",
    "\n",
    "plt.figure(figsize=[20,7])\n",
    "plt.plot(eye_brightness_df.iloc[region].index, eye_brightness_df.iloc[region].right_eye, label='right')\n",
    "plt.plot(eye_brightness_df.iloc[region].index, eye_brightness_df.iloc[region].left_eye, label='left')\n",
    "plt.plot(eye_brightness_df.iloc[region].index, arena_brightness_df.iloc[region].left_20210324T125835, label='arena1')\n",
    "plt.plot(eye_brightness_df.iloc[region].index, arena_brightness_df.iloc[region].stream_20210324T125835, label='arena2')\n",
    "plt.vlines(eye_brightness_df.iloc[region].index, -20, -8, linestyles={'dashed'})\n",
    "plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% explore synchronization\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def four_video_concat(output_name, vid1, vid2, vid3, vid4,  start_frame, shortest_vid_length, format='H264'):\n",
    "    \"\"\"\n",
    "    :param vid1: left down\n",
    "    :param vid2: right down\n",
    "    :param vid3: left up\n",
    "    :param vid4: right up\n",
    "    :param shortest_vid_length:\n",
    "    :param output_name: output file name\n",
    "    :return: a concatenated video of the 4 inputs\n",
    "    \"\"\"\n",
    "    cap0 = cv2.VideoCapture(vid1)\n",
    "    cap1 = cv2.VideoCapture(vid2)\n",
    "    cap2 = cv2.VideoCapture(vid3)\n",
    "    cap3 = cv2.VideoCapture(vid4)\n",
    "    anchor = start_frame\n",
    "    fourcc = cv2.VideoWriter_fourcc(*format)\n",
    "    out = cv2.VideoWriter((path + r'\\\\' + output_name + '.mp4'),fourcc, 60.0, (640*2,480*2))\n",
    "    try:\n",
    "        while cap2.isOpened():\n",
    "            ar_f = sync_vids.Arena_VideoFrame[anchor]\n",
    "            l_eye_f = sync_vids.Left_eye[anchor]\n",
    "            r_eye_f = sync_vids.Right_eye[anchor]\n",
    "\n",
    "            cap0.set(1,ar_f)\n",
    "            ar_ret0, ar_frame0 = cap0.read()\n",
    "            ar_frame0 = cv2.cvtColor(ar_frame0, cv2.COLOR_BGR2GRAY)\n",
    "            ar_frame0 = cv2.resize(ar_frame0,(640,480))\n",
    "\n",
    "            cap1.set(1,ar_f)\n",
    "            ar_ret, ar_frame = cap1.read()\n",
    "            ar_frame = cv2.cvtColor(ar_frame, cv2.COLOR_BGR2GRAY)\n",
    "            ar_frame = cv2.resize(ar_frame,(640,480))\n",
    "\n",
    "            cap2.set(1,l_eye_f)\n",
    "            le_ret, le_f = cap2.read()\n",
    "            le_f = cv2.cvtColor(le_f, cv2.COLOR_BGR2GRAY)\n",
    "            le_f = cv2.flip(le_f, 0)\n",
    "            le_f = cv2.resize(le_f,(640,480))\n",
    "\n",
    "            cap3.set(1,r_eye_f)\n",
    "            re_ret, re_f = cap3.read()\n",
    "            re_f = cv2.cvtColor(re_f, cv2.COLOR_BGR2GRAY)\n",
    "            re_f = cv2.flip(re_f, 0)\n",
    "            re_f = cv2.resize(re_f,(640,480))\n",
    "\n",
    "            eye_concat = np.hstack((le_f,re_f))\n",
    "            ar_concat = np.hstack((ar_frame0, ar_frame))\n",
    "            vconcat = np.vstack((eye_concat, ar_concat))\n",
    "\n",
    "            out.write(vconcat)\n",
    "            anchor += 1\n",
    "            print(f'writing video frame {anchor} out of {shortest_vid_length}', end='\\r', flush=True)\n",
    "            if anchor > shortest_vid_length-1:\n",
    "                break\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    except Exception:\n",
    "        print(f'Encountered a problem with frame {anchor}, stopping concatenation')\n",
    "    finally:\n",
    "        cap0.release()\n",
    "        cap1.release()\n",
    "        cap2.release()\n",
    "        cap3.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print('\\n')\n",
    "        print('Processed finished')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% video concatenation function\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "four_video_concat_faster('block_9_synchronization',arena_videos[1],arena_videos[3],le_video[0],re_video[0],10,24900)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cProfile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cProfile.run(\"four_video_concat('block_8_synchronization_profiler',arena_videos[1],arena_videos[3],le_video[0],re_video[0],15,300)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def four_video_concat_faster(output_name, vid1, vid2, vid3, vid4,  start_frame, shortest_vid_length, format='H264'):\n",
    "    \"\"\"\n",
    "    :param vid1: left down\n",
    "    :param vid2: right down\n",
    "    :param vid3: left up\n",
    "    :param vid4: right up\n",
    "    :param shortest_vid_length:\n",
    "    :param output_name: output file name\n",
    "    :return: a concatenated video of the 4 inputs\n",
    "    \"\"\"\n",
    "    cap0 = cv2.VideoCapture(vid1)\n",
    "    cap1 = cv2.VideoCapture(vid2)\n",
    "    cap2 = cv2.VideoCapture(vid3)\n",
    "    cap3 = cv2.VideoCapture(vid4)\n",
    "    anchor = start_frame\n",
    "    last_ar_f = sync_vids.Arena_VideoFrame[start_frame]\n",
    "    last_le = sync_vids.Left_eye[start_frame]\n",
    "    last_re = sync_vids.Right_eye[start_frame]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*format)\n",
    "    out = cv2.VideoWriter((path + r'\\\\' + output_name + '.mp4'),fourcc, 60.0, (640*2,480*2))\n",
    "    try:\n",
    "        while cap2.isOpened():\n",
    "            ar_f = sync_vids.Arena_VideoFrame[anchor]\n",
    "            l_eye_f = sync_vids.Left_eye[anchor]\n",
    "            r_eye_f = sync_vids.Right_eye[anchor]\n",
    "\n",
    "            if ar_f != last_ar_f + 1:\n",
    "                cap0.set(1,ar_f)\n",
    "            ar_ret0, ar_frame0 = cap0.read()\n",
    "            ar_frame0 = cv2.cvtColor(ar_frame0, cv2.COLOR_BGR2GRAY)\n",
    "            ar_frame0 = cv2.resize(ar_frame0,(640,480))\n",
    "            last_ar_f = ar_f\n",
    "\n",
    "            if ar_f != last_ar_f + 1:\n",
    "                cap1.set(1,ar_f)\n",
    "            ar_ret, ar_frame = cap1.read()\n",
    "            ar_frame = cv2.cvtColor(ar_frame, cv2.COLOR_BGR2GRAY)\n",
    "            ar_frame = cv2.resize(ar_frame,(640,480))\n",
    "\n",
    "            if l_eye_f != last_le + 1:\n",
    "                cap2.set(1,l_eye_f)\n",
    "            le_ret, le_f = cap2.read()\n",
    "            le_f = cv2.cvtColor(le_f, cv2.COLOR_BGR2GRAY)\n",
    "            le_f = cv2.flip(le_f, 0)\n",
    "            le_f = cv2.resize(le_f,(640,480))\n",
    "            last_le = l_eye_f\n",
    "\n",
    "            if r_eye_f != last_re + 1:\n",
    "                cap3.set(1,r_eye_f)\n",
    "            re_ret, re_f = cap3.read()\n",
    "            re_f = cv2.cvtColor(re_f, cv2.COLOR_BGR2GRAY)\n",
    "            re_f = cv2.flip(re_f, 0)\n",
    "            re_f = cv2.resize(re_f,(640,480))\n",
    "            last_re = r_eye_f\n",
    "\n",
    "            eye_concat = np.hstack((le_f,re_f))\n",
    "            ar_concat = np.hstack((ar_frame0, ar_frame))\n",
    "            vconcat = np.vstack((eye_concat, ar_concat))\n",
    "\n",
    "            out.write(vconcat)\n",
    "            anchor += 1\n",
    "            print(f'writing video frame {anchor} out of {shortest_vid_length}', end='\\r', flush=True)\n",
    "            if anchor > shortest_vid_length-1:\n",
    "                break\n",
    "    except Exception:\n",
    "        print(f'Encountered a problem with frame {anchor}, stopping concatenation')\n",
    "    finally:\n",
    "        cap0.release()\n",
    "        cap1.release()\n",
    "        cap2.release()\n",
    "        cap3.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print('\\n')\n",
    "        print('Processed finished')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cProfile.run(\"four_video_concat_faster('block_10_synchronization_profiler_faster',arena_videos[1],arena_videos[3],le_video[0],re_video[0],15,3615)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}